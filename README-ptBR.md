# Processamento de JSON Altamente Comprimidos em Larga Escala na AWS üöÄ

Imagine lidar com um banco de dados OLTP em produ√ß√£o sem indexa√ß√£o, com baixa performance e bilh√µes de registros nas tabelas principais do neg√≥cio. Parece o pesadelo de qualquer engenheiro de dados, certo? Recentemente, enfrentamos exatamente esse desafio em um dos projetos em que atuamos como Engenheiros de Dados.

## üéØ O desafio:
Extrair informa√ß√µes de um banco origem e disponibiliz√°-las para os usu√°rios dentro da AWS, enfrentando a lentid√£o do banco e o tamanho massivo das tabelas.

## üí° O insight:
Durante nossa investiga√ß√£o, descobrimos uma coluna que armazenava as mesmas informa√ß√µes de tabelas com bilh√µes de registros n√£o indexados, mas em formato JSON comprimido em GZIP. Apesar de otimizada para armazenamento, essa coluna continha registros com mais de **800 mil caracteres cada**!

## üîß A solu√ß√£o estrat√©gica:
Optamos por ingerir apenas essa coluna comprimida para o S3, process√°-la e descomprim√≠-la usando **AWS Glue** com **PySpark**. Os dados foram transformados em m√∫ltiplas tabelas organizadas no formato **Iceberg**, otimizando o consumo via **Athena**. Al√©m disso, criamos vis√µes sumarizadas no **Data Warehouse**, tornando os dados mais acess√≠veis e prontos para an√°lise.

## üöÄ Explorando diferentes abordagens:
Testamos descompress√£o e transforma√ß√£o com:
- **Pandas + UDFs do PySpark**: eficiente, mas com sobrecarga de transforma√ß√£o entre Python e JVM.
- **Scala**: eliminou a transforma√ß√£o Python ‚Üí JVM, entregando melhor desempenho.

## üíæ Simula√ß√£o e reprodutibilidade:
Criamos um reposit√≥rio no GitHub para simular o comportamento do banco, gerando arquivos Parquet que replicam a estrutura original. A partir disso, processamos os dados e os disponibilizamos em tabelas **Iceberg** com o cat√°logo no Hadoop, compartilhando pr√°ticas de organiza√ß√£o e performance.

Essa experi√™ncia refor√ßou algo que sempre acredito: mesmo em cen√°rios adversos, com an√°lise detalhada e escolhas estrat√©gicas, √© poss√≠vel encontrar solu√ß√µes robustas para problemas complexos de dados.

Se voc√™ j√° enfrentou desafios similares, compartilhe nos coment√°rios! Vamos trocar ideias e experi√™ncias! üöÄüí¨

## üìñ O passo a passo

### 1. UDF
Aqui mostramos o c√≥digo da UDF em Scala e a compila√ß√£o manual do arquivo JAR. Essa se√ß√£o √© opcional e pode-se utilizar o arquivo
pr√©-compilado dispon√≠vel no reposit√≥rio.

#### 1.1 C√≥digo Scala

O PySpark precisa de um arquivo JAR contendo nossa UDF em Scala para poder execut√°-la. Ent√£o, come√ßando com um projeto Scala
b√°sico, crie um arquivo chamado *Decompress.scala* contendo o seguinte c√≥digo:

  ```scala
  import org.apache.spark.sql.api.java.UDF1
  import java.io.ByteArrayInputStream
  import java.util.zip.GZIPInputStream

  class Decompress extends UDF1[Array[Byte], Array[Byte]] {
    override def call(t1: Array[Byte]): Array[Byte] = {
      val inputStream = new GZIPInputStream(new ByteArrayInputStream(t1))
      org.apache.commons.io.IOUtils.toByteArray(inputStream)
    }
  }
  ```

Note que nossa classe Decompress est√° estendendo a UDF1 do Spark que aceita um argumento, neste caso
um array de bytes (dados comprimidos) e tamb√©m esperamos que a sa√≠da seja um array de bytes (dados n√£o comprimidos).

Agora que temos nosso c√≥digo completo, podemos compil√°-lo com o sbt:

`sbt clean package`

O arquivo JAR necess√°rio encontra-se dentro da pasta target do seu projeto.

#### 1.2 C√≥digo Python

Para permitir que o PySpark reconhe√ßa nossa UDF, precisamos adicionar o arquivo JAR quando criamos nossa Spark session:

```python
spark = (SparkSession.builder
.config(‚Äúspark.jars‚Äù, ‚Äú./scala_udf.jar‚Äù)
.getOrCreate())
```

Agora vamos regist√°-lo no contexto SQL para podermos cham√°-lo com o Spark SQL:

```python
sqlContext = SQLContext(spark.sparkContext)
spark.udf.registerJavaFunction(‚Äúdecompress_scala‚Äù, ‚ÄúDescompactar‚Äù, T.BinaryType())
```
Aqui decompress_scala ser√° o nome registrado no contexto SQL, Decompress √© o nome da classe da nossa UDF e BinaryType
refere-se ao seu tipo de retorno.

Para us√°-lo, basta adicionar a seguinte linha de c√≥digo:
```python
df = spark.sql(‚Äúselect *, decode(decompress_scala(compressed_json), ‚Äòutf8‚Äô) as json from df‚Äù)
```
Aqui, usamos decode para converter nossa sa√≠da bin√°ria em uma string utf-8, pois isso era necess√°rio no exemplo escolhido.

#### 1.3 Resultados da execu√ß√£o
Lendo o arquivo data.parquet fornecido, descompactamos e decodificamos uma coluna JSON gzipada:

![Decompress result](./img.png)

### 2. C√≥digo principal
#### 2.1 Recursos

- Gera√ß√£o de conjuntos de dados fict√≠cios para clientes, pedidos e pagamentos usando `Faker`.
- Armazenamento de dados no formato JSON compactado dentro de arquivos Parquet.
- Descompacta√ß√£o e transforma√ß√£o de dados JSON em tabelas estruturadas com PySpark.
- Carregamento dos dados processados em tabelas Iceberg com esquemas predefinidos.
- Suporte √† imposi√ß√£o de esquemas e fluxos de ETL escal√°veis.

#### 2.2 Instala√ß√£o

1. Clone o reposit√≥rio:
   ```bash
   git clone https://github.com/leonardovsramos/decompress-json-etl.git
   ```

2. Navegue at√© o diret√≥rio do projeto:
   ```bash
   cd decompress-json-etl
   ```

3. Instale as depend√™ncias usando `uv`:
   ```bash
   uv setup
   ```

---

#### 2.3 Uso

##### 2.3.1 Gera√ß√£o de dados fict√≠cios
Execute o script para gerar dados JSON compactados:
```bash
python generate_json.py --num_orders <numero_de_pedidos> --output_path <caminho_do_arquivo_de_saida>
```

**Exemplo:**
```bash
python generate_json.py --num_orders 100 --output_path ./data
```

##### 2.3.2 Processar dados em tabelas Iceberg
Use o script ETL para descompactar e carregar os dados nas tabelas Iceberg:
```bash
python process_json.py --input_path <caminho_do_arquivo_de_entrada>
```

**Exemplo:**
```bash
python process_json.py --input_path ./data/compressed_json.parquet
```

---

#### 2.4 Como funciona

##### 2.4.1 Gera√ß√£o de dados
- Dados fict√≠cios para clientes, pedidos e pagamentos s√£o criados usando a biblioteca `Faker`.
- Os dados s√£o compactados no formato JSON e armazenados em arquivos Parquet.

##### 2.4.2 Transforma√ß√£o de dados
- O JSON compactado √© descompactado usando uma UDF em Scala.
- O PySpark transforma os dados JSON em registros estruturados para:
    - Pedidos
    - Clientes
    - Itens do Pedido
    - Pagamentos

##### 2.4.3 Carregamento de dados
- Os dados transformados s√£o carregados em tabelas Iceberg, aplicando esquemas e propriedades pr√©-definidos.

#### 2.5 Defini√ß√µes de esquema

##### 2.5.1 Tabela de pedidos (Orders)
| Coluna        | Tipo               | Descri√ß√£o                    |
|---------------|:------------------:|-----------------------------|
| order_id      | INT                | Identificador √∫nico do pedido. |
| order_date    | TIMESTAMP          | Data e hora do pedido.      |
| order_price   | DECIMAL(15,2)      | Pre√ßo total do pedido.      |

##### 2.5.2 Tabela de clientes (Clients)
| Coluna        | Tipo               | Descri√ß√£o                   |
|---------------|:------------------:|-----------------------------|
| order_id      | INT               | Identificador vinculado ao pedido. |
| client_id     | INT               | Identificador √∫nico do cliente. |
| first_name    | STRING            | Primeiro nome do cliente.  |
| last_name     | STRING            | Sobrenome do cliente.      |
| cpf           | STRING            | CPF brasileiro do cliente. |
| rg            | STRING            | RG brasileiro do cliente.  |
| email         | STRING            | Endere√ßo de e-mail do cliente. |
| birth_date    | DATE              | Data de nascimento do cliente. |

##### 2.5.3 Tabela de itens do pedido (Order Items)
| Coluna           | Tipo               | Descri√ß√£o                   |
|------------------|:------------------:|-----------------------------|
| order_id         | INT                | Identificador vinculado ao pedido. |
| id               | INT                | Identificador √∫nico do item. |
| item_name        | STRING             | Nome do item.              |
| item_price       | DECIMAL(15,2)      | Pre√ßo do item.             |
| item_description | STRING             | Descri√ß√£o do item.         |

##### 2.5.4 Tabela de pagamentos (Payments)
| Coluna                    | Tipo               | Descri√ß√£o                   |
|---------------------------|:------------------:|-----------------------------|
| order_id                  | INT                | Identificador vinculado ao pedido. |
| id                        | INT                | Identificador √∫nico do pagamento. |
| method                    | STRING             | M√©todo de pagamento (dinheiro, cart√£o de cr√©dito, etc.). |
| amount                    | DECIMAL(15,2)      | Valor total pago.           |
| tranch_value              | DECIMAL(15,2)      | Valor de cada parcela (se aplic√°vel). |
| tranch_payment_date       | DATE               | Data do pagamento da parcela. |
| tranch_installment_number | SMALLINT           | N√∫mero da parcela.          |

## Reposit√≥rio no GitHub
https://github.com/leonardovsramos/decompress-json-etl

## Refer√™ncias
https://spark.apache.org/docs/3.5.1/api/java/org/apache/spark/sql/api/java/UDF1.html
https://spark.apache.org/docs/3.5.2/sql-ref-functions-udf-scalar.html

## Autores
[Leonardo Vieira dos Santos Ramos](https://www.linkedin.com/in/leonardolvsr/)

[V√≠tor Rodrigues G√¥ngora](https://www.linkedin.com/in/vitorgongora/)