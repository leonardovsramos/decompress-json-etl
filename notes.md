# Processamento de JSON Altamente Comprimidos em Larga Escala na AWS ğŸš€

Imagine lidar com um banco de dados OLTP em produÃ§Ã£o sem indexaÃ§Ã£o, com baixa performance e bilhÃµes de registros nas tabelas principais do negÃ³cio. Parece o pesadelo de qualquer engenheiro de dados, certo? Recentemente, enfrentei exatamente esse desafio em um dos projetos em que atuei como Engenheiro de Dados.

## ğŸ¯ O desafio:  
Extrair informaÃ§Ãµes desse banco de origem e disponibilizÃ¡-las para os usuÃ¡rios dentro da AWS, enfrentando a lentidÃ£o e o tamanho massivo das tabelas.

## ğŸ’¡ O insight:  
Durante nossa investigaÃ§Ã£o, descobrimos uma coluna que armazenava as mesmas informaÃ§Ãµes das tabelas com bilhÃµes de registros, mas em formato JSON comprimido em GZIP. Apesar de otimizada para armazenamento, essa coluna continha registros com mais de **800 mil caracteres cada**!

## ğŸ”§ A soluÃ§Ã£o estratÃ©gica:  
Optamos por ingerir apenas essa coluna comprimida para o S3, processÃ¡-la e descomprimÃ­-la usando **AWS Glue** com **PySpark**. Os dados foram transformados em mÃºltiplas tabelas organizadas no formato **Iceberg**, otimizando o consumo via **Athena**. AlÃ©m disso, criamos visÃµes sumarizadas no **Data Warehouse**, tornando os dados mais acessÃ­veis e prontos para anÃ¡lise.

## ğŸš€ Explorando diferentes abordagens:  
Testamos descompressÃ£o e transformaÃ§Ã£o com:
- **Pandas + UDFs do PySpark**: eficiente, mas com sobrecarga de transformaÃ§Ã£o entre Python e JVM.  
- **Scala**: eliminou a transformaÃ§Ã£o Python â†’ JVM, entregando melhor performance.

## ğŸ’¾ SimulaÃ§Ã£o e reprodutibilidade:  
Criamos um repositÃ³rio no GitHub para simular o comportamento do banco, gerando arquivos Parquet que replicam a estrutura original. A partir disso, processamos os dados e os disponibilizamos em tabelas **Iceberg** com o catÃ¡logo no Hadoop, compartilhando prÃ¡ticas de organizaÃ§Ã£o e performance.

Essa experiÃªncia reforÃ§ou algo que sempre acredito: mesmo em cenÃ¡rios adversos, com anÃ¡lise detalhada e escolhas estratÃ©gicas, Ã© possÃ­vel encontrar soluÃ§Ãµes robustas para problemas complexos de dados.

Se vocÃª jÃ¡ enfrentou desafios similares, compartilhe nos comentÃ¡rios! Vamos trocar ideias e experiÃªncias! ğŸš€ğŸ’¬  

----

# Processing Highly Compressed JSON at Scale on AWS ğŸš€

Imagine dealing with a production OLTP database with no indexing, poor performance, and billions of records in the main business tables. Sounds like a nightmare for any data engineer, right? Recently, I faced exactly this challenge in one of the projects where I worked as a Data Engineer.

## ğŸ¯ The Challenge:  
Extract information from this source database and make it available to users within AWS, despite the sluggish performance and massive size of the tables.

## ğŸ’¡ The Insight:  
During our investigation, we discovered a column that stored the same information as the tables with billions of records but in JSON format compressed with **GZIP**. While optimized for storage, this column contained records with more than **800,000 characters each**!

## ğŸ”§ The Strategic Solution:  
We decided to ingest only this compressed column into **S3**, process it, and decompress it using **AWS Glue** with **PySpark**. The data was transformed into multiple tables organized in the **Iceberg** format, optimizing consumption through **Athena**. Additionally, we created summarized views in the **Data Warehouse**, making the data more accessible and analysis-ready.

## ğŸš€ Exploring Different Approaches:  
We tested decompression and transformation using:
- **Pandas + PySpark UDFs**: efficient but with the overhead of transforming between Python and the JVM.  
- **Scala**: eliminated the Python â†’ JVM transformation, resulting in better performance.

## ğŸ’¾ Simulation and Reproducibility:  
We created a GitHub repository to simulate the database behavior by generating Parquet files that replicate the original structure. From there, we processed the data and made it available in **Iceberg** tables using a Hadoop catalog, sharing best practices for organization and performance.

This experience reinforced something I always believe: even in adverse scenarios, with detailed analysis and strategic decisions, it is possible to find robust solutions for complex data processing challenges.

If youâ€™ve faced similar challenges, share your experiences in the comments! Letâ€™s exchange ideas and insights! ğŸš€ğŸ’¬
