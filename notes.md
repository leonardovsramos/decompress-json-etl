# Processamento de JSON Altamente Comprimidos em Larga Escala na AWS üöÄ

Imagine lidar com um banco de dados OLTP em produ√ß√£o sem indexa√ß√£o, com baixa performance e bilh√µes de registros nas tabelas principais do neg√≥cio. Parece o pesadelo de qualquer engenheiro de dados, certo? Recentemente, enfrentamos exatamente esse desafio em um dos projetos em que atuamos como Engenheiros de Dados.

## üéØ O desafio:  
Extrair informa√ß√µes de um banco origem e disponibiliz√°-las para os usu√°rios dentro da AWS, enfrentando a lentid√£o do banco e o tamanho massivo das tabelas.

## üí° O insight:  
Durante nossa investiga√ß√£o, descobrimos uma coluna que armazenava as mesmas informa√ß√µes de tabelas com bilh√µes de registros n√£o indexados, mas em formato JSON comprimido em GZIP. Apesar de otimizada para armazenamento, essa coluna continha registros com mais de **800 mil caracteres cada**!

## üîß A solu√ß√£o estrat√©gica:  
Optamos por ingerir apenas essa coluna comprimida para o S3, process√°-la e descomprim√≠-la usando **AWS Glue** com **PySpark**. Os dados foram transformados em m√∫ltiplas tabelas organizadas no formato **Iceberg**, otimizando o consumo via **Athena**. Al√©m disso, criamos vis√µes sumarizadas no **Data Warehouse**, tornando os dados mais acess√≠veis e prontos para an√°lise.

## üöÄ Explorando diferentes abordagens:  
Testamos descompress√£o e transforma√ß√£o com:
- **Pandas + UDFs do PySpark**: eficiente, mas com sobrecarga de transforma√ß√£o entre Python e JVM.  
- **Scala**: eliminou a transforma√ß√£o Python ‚Üí JVM, entregando melhor desempenho.

## üíæ Simula√ß√£o e reprodutibilidade:  
Criamos um reposit√≥rio no GitHub para simular o comportamento do banco, gerando arquivos Parquet que replicam a estrutura original. A partir disso, processamos os dados e os disponibilizamos em tabelas **Iceberg** com o cat√°logo no Hadoop, compartilhando pr√°ticas de organiza√ß√£o e performance.

Essa experi√™ncia refor√ßou algo que sempre acredito: mesmo em cen√°rios adversos, com an√°lise detalhada e escolhas estrat√©gicas, √© poss√≠vel encontrar solu√ß√µes robustas para problemas complexos de dados.

Se voc√™ j√° enfrentou desafios similares, compartilhe nos coment√°rios! Vamos trocar ideias e experi√™ncias! üöÄüí¨  

## üìñ O passo a passo

### 1. UDF
Aqui mostramos o c√≥digo da UDF em Scala e a compila√ß√£o manual do arquivo JAR. Essa se√ß√£o √© opcional e pode-se utilizar o arquivo
pr√©-compilado dispon√≠vel no reposit√≥rio.

#### 1.1 C√≥digo Scala

O PySpark precisa de um arquivo JAR contendo nossa UDF em Scala para poder execut√°-la. Ent√£o, come√ßando com um projeto Scala
b√°sico, crie um arquivo chamado *Decompress.scala* contendo o seguinte c√≥digo:

  ```scala
  import org.apache.spark.sql.api.java.UDF1
  import java.io.ByteArrayInputStream
  import java.util.zip.GZIPInputStream

  class Decompress extends UDF1[Array[Byte], Array[Byte]] {
    override def call(t1: Array[Byte]): Array[Byte] = {
      val inputStream = new GZIPInputStream(new ByteArrayInputStream(t1))
      org.apache.commons.io.IOUtils.toByteArray(inputStream)
    }
  }
  ```

Note que nossa classe Decompress est√° estendendo a UDF1 do Spark que aceita um argumento, neste caso
um array de bytes (dados comprimidos) e tamb√©m esperamos que a sa√≠da seja um array de bytes (dados n√£o comprimidos).

Agora que temos nosso c√≥digo completo, podemos compil√°-lo com o sbt:
  
`sbt clean package`
   
O arquivo JAR necess√°rio encontra-se dentro da pasta target do seu projeto.

#### 1.2 C√≥digo Python

Para permitir que o PySpark reconhe√ßa nossa UDF, precisamos adicionar o arquivo JAR quando criamos nossa Spark session:

```python
spark = (SparkSession.builder
.config(‚Äúspark.jars‚Äù, ‚Äú./scala_udf.jar‚Äù)
.getOrCreate())
```

Agora vamos regist√°-lo no contexto SQL para podermos cham√°-lo com o Spark SQL:

```python
sqlContext = SQLContext(spark.sparkContext)
spark.udf.registerJavaFunction(‚Äúdecompress_scala‚Äù, ‚ÄúDescompactar‚Äù, T.BinaryType())
```
Aqui decompress_scala ser√° o nome registrado no contexto SQL, Decompress √© o nome da classe da nossa UDF e BinaryType
refere-se ao seu tipo de retorno.

Para us√°-lo, basta adicionar a seguinte linha de c√≥digo:
```python
df = spark.sql(‚Äúselect *, decode(decompress_scala(compressed_json), ‚Äòutf8‚Äô) as json from df‚Äù)
```
Aqui, usamos decode para converter nossa sa√≠da bin√°ria em uma string utf-8, pois isso era necess√°rio no exemplo escolhido.

#### 1.3 Resultados da execu√ß√£o
Lendo o arquivo data.parquet fornecido, descompactamos e decodificamos uma coluna JSON gzipada:

![Decompress result](./img.png)

### 2. C√≥digo principal
#### 2.1 Recursos

- Gera√ß√£o de conjuntos de dados fict√≠cios para clientes, pedidos e pagamentos usando `Faker`.
- Armazenamento de dados no formato JSON compactado dentro de arquivos Parquet.
- Descompacta√ß√£o e transforma√ß√£o de dados JSON em tabelas estruturadas com PySpark.
- Carregamento dos dados processados em tabelas Iceberg com esquemas predefinidos.
- Suporte √† imposi√ß√£o de esquemas e fluxos de ETL escal√°veis.

#### 2.2 Instala√ß√£o

1. Clone o reposit√≥rio:
   ```bash
   git clone https://github.com/leonardovsramos/decompress-json-etl.git
   ```

2. Navegue at√© o diret√≥rio do projeto:
   ```bash
   cd decompress-json-etl
   ```

3. Instale as depend√™ncias usando `uv`:
   ```bash
   uv setup
   ```

---

#### 2.3 Uso

##### 2.3.1 Gera√ß√£o de dados fict√≠cios
Execute o script para gerar dados JSON compactados:
```bash
python generate_json.py --num_orders <numero_de_pedidos> --output_path <caminho_do_arquivo_de_saida>
```

**Exemplo:**
```bash
python generate_json.py --num_orders 100 --output_path ./data
```

##### 2.3.2 Processar dados em tabelas Iceberg
Use o script ETL para descompactar e carregar os dados nas tabelas Iceberg:
```bash
python process_json.py --input_path <caminho_do_arquivo_de_entrada>
```

**Exemplo:**
```bash
python process_json.py --input_path ./data/compressed_json.parquet
```

---

#### 2.4 Como funciona

##### 2.4.1 Gera√ß√£o de dados
- Dados fict√≠cios para clientes, pedidos e pagamentos s√£o criados usando a biblioteca `Faker`.
- Os dados s√£o compactados no formato JSON e armazenados em arquivos Parquet.

##### 2.4.2 Transforma√ß√£o de dados
- O JSON compactado √© descompactado usando uma UDF em Scala.
- O PySpark transforma os dados JSON em registros estruturados para:
    - Pedidos
    - Clientes
    - Itens do Pedido
    - Pagamentos

##### 2.4.3 Carregamento de dados
- Os dados transformados s√£o carregados em tabelas Iceberg, aplicando esquemas e propriedades pr√©-definidos.

#### 2.5 Defini√ß√µes de esquema

##### 2.5.1 Tabela de pedidos (Orders)
| Coluna        | Tipo               | Descri√ß√£o                    |
|---------------|:------------------:|-----------------------------|
| order_id      | INT                | Identificador √∫nico do pedido. |
| order_date    | TIMESTAMP          | Data e hora do pedido.      |
| order_price   | DECIMAL(15,2)      | Pre√ßo total do pedido.      |

##### 2.5.2 Tabela de clientes (Clients)
| Coluna        | Tipo               | Descri√ß√£o                   |
|---------------|:------------------:|-----------------------------|
| order_id      | INT               | Identificador vinculado ao pedido. |
| client_id     | INT               | Identificador √∫nico do cliente. |
| first_name    | STRING            | Primeiro nome do cliente.  |
| last_name     | STRING            | Sobrenome do cliente.      |
| cpf           | STRING            | CPF brasileiro do cliente. |
| rg            | STRING            | RG brasileiro do cliente.  |
| email         | STRING            | Endere√ßo de e-mail do cliente. |
| birth_date    | DATE              | Data de nascimento do cliente. |

##### 2.5.3 Tabela de itens do pedido (Order Items)
| Coluna           | Tipo               | Descri√ß√£o                   |
|------------------|:------------------:|-----------------------------|
| order_id         | INT                | Identificador vinculado ao pedido. |
| id               | INT                | Identificador √∫nico do item. |
| item_name        | STRING             | Nome do item.              |
| item_price       | DECIMAL(15,2)      | Pre√ßo do item.             |
| item_description | STRING             | Descri√ß√£o do item.         |

##### 2.5.4 Tabela de pagamentos (Payments)
| Coluna                    | Tipo               | Descri√ß√£o                   |
|---------------------------|:------------------:|-----------------------------|
| order_id                  | INT                | Identificador vinculado ao pedido. |
| id                        | INT                | Identificador √∫nico do pagamento. |
| method                    | STRING             | M√©todo de pagamento (dinheiro, cart√£o de cr√©dito, etc.). |
| amount                    | DECIMAL(15,2)      | Valor total pago.           |
| tranch_value              | DECIMAL(15,2)      | Valor de cada parcela (se aplic√°vel). |
| tranch_payment_date       | DATE               | Data do pagamento da parcela. |
| tranch_installment_number | SMALLINT           | N√∫mero da parcela.          |

## Reposit√≥rio no GitHub
https://github.com/leonardovsramos/decompress-json-etl

## Refer√™ncias
https://spark.apache.org/docs/3.5.1/api/java/org/apache/spark/sql/api/java/UDF1.html
https://spark.apache.org/docs/3.5.2/sql-ref-functions-udf-scalar.html

## Autores
[Leonardo Vieira dos Santos Ramos](https://www.linkedin.com/in/leonardolvsr/)

[V√≠tor Rodrigues G√¥ngora](https://www.linkedin.com/in/vitorgongora/)

----

# Processing Highly Compressed JSON at Scale on AWS üöÄ

Imagine dealing with a production OLTP database with no indexing, poor performance, and billions of records in the main business tables. Sounds like a nightmare for any data engineer, right? Recently, I faced exactly this challenge in one of the projects where I worked as a Data Engineer.

## üéØ The Challenge:  
Extract information from this source database and make it available to users within AWS, despite the sluggish performance and massive size of the tables.

## üí° The Insight:  
During our investigation, we discovered a column that stored the same information as the tables with billions of records but in JSON format compressed with **GZIP**. While optimized for storage, this column contained records with more than **800,000 characters each**!

## üîß The Strategic Solution:  
We decided to ingest only this compressed column into **S3**, process it, and decompress it using **AWS Glue** with **PySpark**. The data was transformed into multiple tables organized in the **Iceberg** format, optimizing consumption through **Athena**. Additionally, we created summarized views in the **Data Warehouse**, making the data more accessible and analysis-ready.

## üöÄ Exploring Different Approaches:  
We tested decompression and transformation using:
- **Pandas + PySpark UDFs**: efficient but with the overhead of transforming between Python and the JVM.  
- **Scala**: eliminated the Python ‚Üí JVM transformation, resulting in better performance.

## üíæ Simulation and Reproducibility:  
We created a GitHub repository to simulate the database behavior by generating Parquet files that replicate the original structure. From there, we processed the data and made it available in **Iceberg** tables using a Hadoop catalog, sharing best practices for organization and performance.

This experience reinforced something I always believe: even in adverse scenarios, with detailed analysis and strategic decisions, it is possible to find robust solutions for complex data processing challenges.

If you‚Äôve faced similar challenges, share your experiences in the comments! Let‚Äôs exchange ideas and insights! üöÄüí¨
